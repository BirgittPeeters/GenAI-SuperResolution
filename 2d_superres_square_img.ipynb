{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xy5Q1IV38T9h",
        "outputId": "2cd99ef9-0c87-4ab6-bb55-e75a9ed612de"
      },
      "outputs": [],
      "source": [
        "!pip install -q \"monai-weekly[tqdm]\"\n",
        "!python -c \"import generative\" || pip install -q monai-generative\n",
        "!python -c \"import lpips\" || pip install -q lpips\n",
        "!python -c \"import pydicom\" || pip install -q pydicom\n",
        "!python -c \"import matplotlib\" || pip install -q matplotlib\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYsYFIrF81cw",
        "outputId": "94526d3a-70f7-41d8-a1ea-787202102b54"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\paramiko\\transport.py:219: CryptographyDeprecationWarning: Blowfish has been deprecated\n",
            "  \"class\": algorithms.Blowfish,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MONAI version: 1.4.dev2404\n",
            "Numpy version: 1.24.3\n",
            "Pytorch version: 2.1.2+cu118\n",
            "MONAI flags: HAS_EXT = False, USE_COMPILED = False, USE_META_DICT = False\n",
            "MONAI rev id: 3ebfa1ee9980ae588ac19164d797684069af4c5a\n",
            "MONAI __file__: C:\\Users\\<username>\\AppData\\Roaming\\Python\\Python311\\site-packages\\monai\\__init__.py\n",
            "\n",
            "Optional dependencies:\n",
            "Pytorch Ignite version: 0.4.11\n",
            "ITK version: 5.3.0\n",
            "Nibabel version: 5.2.0\n",
            "scikit-image version: 0.20.0\n",
            "scipy version: 1.10.1\n",
            "Pillow version: 9.4.0\n",
            "Tensorboard version: 2.15.1\n",
            "gdown version: 4.6.3\n",
            "TorchVision version: 0.16.2+cpu\n",
            "tqdm version: 4.65.0\n",
            "lmdb version: 1.4.1\n",
            "psutil version: 5.9.0\n",
            "pandas version: 1.5.3\n",
            "einops version: 0.7.0\n",
            "transformers version: 4.37.0\n",
            "mlflow version: 2.9.2\n",
            "pynrrd version: 1.0.0\n",
            "clearml version: 1.14.2rc0\n",
            "\n",
            "For details about installing the optional dependencies, please visit:\n",
            "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "import tempfile\n",
        "import subprocess\n",
        "import random\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from monai import transforms\n",
        "from monai.apps import MedNISTDataset\n",
        "from monai.config import print_config\n",
        "from monai.data import CacheDataset, DataLoader, Dataset\n",
        "from monai.utils import first, set_determinism\n",
        "from torch import nn\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "from tqdm import tqdm\n",
        "\n",
        "from generative.losses import PatchAdversarialLoss, PerceptualLoss\n",
        "from generative.networks.nets import AutoencoderKL, DiffusionModelUNet, PatchDiscriminator\n",
        "from generative.networks.schedulers import DDPMScheduler\n",
        "\n",
        "print_config()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4lbt9rxY-RDh",
        "outputId": "b240fcae-feea-498f-9398-f30cc119d60c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "C:\\Users\\BIRGIT~1\\AppData\\Local\\Temp\\tmpxsnbdhuj\n"
          ]
        }
      ],
      "source": [
        "directory = os.environ.get(\"MONAI_DATA_DIRECTORY\")\n",
        "root_dir = tempfile.mkdtemp() if directory is None else directory\n",
        "print(root_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWaNSpcL-X1i",
        "outputId": "0a846467-cc37-47cd-828e-41b61331f076"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet idc-index\n",
        "import pydicom\n",
        "import subprocess\n",
        "import random\n",
        "from pydicom.pixel_data_handlers.util import apply_voi_lut\n",
        "from idc_index import index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "q82HLpRl-pJI"
      },
      "outputs": [],
      "source": [
        "# IDC data download script\n",
        "collection_id = \"cmb_lca\"  # Replace with the actual collection ID\n",
        "client = index.IDCClient()\n",
        "query = f\"\"\"\n",
        "SELECT\n",
        "  series_aws_url\n",
        "FROM\n",
        "  index\n",
        "WHERE\n",
        "  Modality = 'MR'\n",
        "  AND collection_id = '{collection_id}'\n",
        "\"\"\"\n",
        "result = client.sql_query(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "uAs1T22R-tBY"
      },
      "outputs": [],
      "source": [
        "# Extract S3 URLs\n",
        "s3_urls = result.series_aws_url.tolist()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ai71dTAl-t5r"
      },
      "outputs": [],
      "source": [
        "# Randomly select a subset for training and validation\n",
        "random.seed(42)  # Set a seed for reproducibility\n",
        "random.shuffle(s3_urls)\n",
        "\n",
        "train_size = 40\n",
        "validation_size = 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "7imcgae6-zk4"
      },
      "outputs": [],
      "source": [
        "train_urls = s3_urls[:train_size]\n",
        "validation_urls = s3_urls[train_size:train_size + validation_size]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Y2rnrjZG-0P5"
      },
      "outputs": [],
      "source": [
        "# Download DICOM files for training set\n",
        "train_download_dir = os.path.join(root_dir, \"MR_DICOM\", \"train\")\n",
        "os.makedirs(train_download_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_SOISkR0-5hZ",
        "outputId": "fb0746af-5e69-4db3-8566-bf7923785a47"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading DICOM files for training set: 100%|██████████| 40/40 [01:13<00:00,  1.83s/it]\n"
          ]
        }
      ],
      "source": [
        "for s3_url in tqdm(train_urls, desc=\"Downloading DICOM files for training set\"):\n",
        "    result = subprocess.run([client.s5cmdPath, '--no-sign-request', 'cp', s3_url, train_download_dir], stdout=subprocess.PIPE)\n",
        "    output = result.stdout.decode('utf-8')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "z0ixDD9d_CnY"
      },
      "outputs": [],
      "source": [
        "# Download DICOM files for validation set\n",
        "validation_download_dir = os.path.join(root_dir, \"MR_DICOM\", \"validation\")\n",
        "os.makedirs(validation_download_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OxlVJRsI_DLJ",
        "outputId": "f0145497-118d-4e5a-de3c-ca518e959276"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading DICOM files for validation set:  22%|██▏       | 6/27 [00:11<00:41,  1.95s/it]"
          ]
        }
      ],
      "source": [
        "for s3_url in tqdm(validation_urls, desc=\"Downloading DICOM files for validation set\"):\n",
        "    result = subprocess.run([client.s5cmdPath, '--no-sign-request', 'cp', s3_url, validation_download_dir], stdout=subprocess.PIPE)\n",
        "    output = result.stdout.decode('utf-8')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EFnNIrRo_Fzw"
      },
      "outputs": [],
      "source": [
        "# Custom DICOM Dataset class\n",
        "class CustomDICOMDataset:\n",
        "    def __init__(self, root_dir, spatial_size=(64, 64)):\n",
        "        self.root_dir = root_dir\n",
        "        self.class_names = self._find_classes()\n",
        "        self.spatial_size = spatial_size\n",
        "        self.data = self._load_dataset()\n",
        "        self.max_image_size = self._find_max_image_size()\n",
        "\n",
        "    def _find_classes(self):\n",
        "        class_names = [d for d in os.listdir(self.root_dir) if os.path.isdir(os.path.join(self.root_dir, d))]\n",
        "        return class_names\n",
        "\n",
        "    def _load_dataset(self):\n",
        "        data = []  # List to store file paths and class names\n",
        "        for class_name in self.class_names:\n",
        "            class_path = os.path.join(self.root_dir, class_name)\n",
        "            for file in os.listdir(class_path):\n",
        "                if file.endswith(\".dcm\"):\n",
        "                    file_path = os.path.join(class_path, file)\n",
        "                    data.append({\"image\": file_path, \"class_name\": class_name})\n",
        "        return data\n",
        "\n",
        "    def _find_max_image_size(self):\n",
        "        max_size = [0, 0]\n",
        "        for item in self.data:\n",
        "            dcm = pydicom.dcmread(item['image'])\n",
        "            max_size[0] = max(max_size[0], dcm.Rows)\n",
        "            max_size[1] = max(max_size[1], dcm.Columns)\n",
        "        return tuple(max_size)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        item = self.data[index]\n",
        "        class_name = item['class_name']\n",
        "        return {'file_path': item['image'], 'class_name': class_name}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ZeLz6cdAXE5"
      },
      "outputs": [],
      "source": [
        "# Create a custom dataset\n",
        "train_data = CustomDICOMDataset(os.path.join(root_dir, \"MR_DICOM\"))\n",
        "max_image_size = train_data.max_image_size\n",
        "# max_res = max(max_image_size)\n",
        "max_res = 256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p2SUEx5cAj9A"
      },
      "outputs": [],
      "source": [
        "train_datalist = [{\"image\": item[\"image\"]} for item in train_data.data if item[\"class_name\"] == \"train\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ueQvSYjsCY6y"
      },
      "outputs": [],
      "source": [
        "train_transforms = transforms.Compose([\n",
        "    transforms.LoadImageD(keys=[\"image\"]),  # Use LoadImageD for DICOM files\n",
        "    transforms.EnsureChannelFirstd(keys=[\"image\"]),\n",
        "    transforms.SqueezeDimd(keys=[\"image\"], dim=-1),\n",
        "    transforms.Resized(keys=[\"image\"],spatial_size=(max_res, max_res)), \n",
        "    transforms.NormalizeIntensityd(keys=[\"image\"]),\n",
        "    transforms.RandAffined(\n",
        "        keys=[\"image\"],\n",
        "        rotate_range=[(-np.pi / 36, np.pi / 36), (-np.pi / 36, np.pi / 36)],\n",
        "        translate_range=[(-1, 1), (-1, 1)],\n",
        "        scale_range=[(0.95, 1.05), (0.95, 1.05)],\n",
        "        spatial_size=(max_res, max_res),  # Adjusted spatial_size for 2D\n",
        "        padding_mode=\"border\",\n",
        "        prob=0.5,\n",
        "    ),\n",
        "    transforms.CopyItemsd(keys=[\"image\"], times=1, names=[\"low_res_image\"]),\n",
        "    transforms.Resized(keys=[\"low_res_image\"], spatial_size=(max_res/4, max_res/4)),\n",
        "])\n",
        "\n",
        "# Apply the transformation to the dataset without caching\n",
        "train_ds = Dataset(data=train_datalist, transform=train_transforms)\n",
        "sample = train_ds[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 887
        },
        "id": "-9fKhCSRCnKy",
        "outputId": "0185c352-91a9-4bc6-e841-8ac24370b572"
      },
      "outputs": [],
      "source": [
        "#%% extra check for dataset\n",
        "sample = train_ds[0]\n",
        "original_image = sample[\"image\"]\n",
        "low_res_image = sample[\"low_res_image\"]\n",
        "\n",
        "plt.imshow(original_image[0, :, :], cmap=\"gray\")\n",
        "plt.title(\"Original Image\")\n",
        "plt.show()\n",
        "\n",
        "plt.imshow(low_res_image[0,:,:], cmap=\"gray\")\n",
        "plt.title(\"Low resolution Image\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "id": "SyesawFTJ0vh",
        "outputId": "8fea36f6-b9e7-434a-c843-953823df0405"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_ds, batch_size=5, shuffle=True, num_workers=0)\n",
        "# # Apply the transformation to the dataset\n",
        "# train_ds = CacheDataset(data=train_datalist, transform=train_transforms)\n",
        "# train_loader = DataLoader(train_ds, batch_size=5, shuffle=True, num_workers=4, persistent_workers=True)\n",
        "# %%\n",
        "#Plot 3 examples from the training set\n",
        "check_data = first(train_loader)\n",
        "fig, ax = plt.subplots(nrows=1, ncols=3)\n",
        "for i in range(3):\n",
        "    ax[i].imshow(check_data[\"image\"][i, 0, :, :], cmap=\"gray\")\n",
        "    ax[i].axis(\"off\")\n",
        "\n",
        "# %%\n",
        "# Plot 3 examples from the training set in low resolution\n",
        "fig, ax = plt.subplots(nrows=1, ncols=3)\n",
        "for i in range(3):\n",
        "    ax[i].imshow(check_data[\"low_res_image\"][i, 0, :, :], cmap=\"gray\")\n",
        "    ax[i].axis(\"off\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJghFtScKTeU"
      },
      "outputs": [],
      "source": [
        "# ## Create data loader for validation set\n",
        "val_data = CustomDICOMDataset(os.path.join(root_dir, \"MR_DICOM\"))\n",
        "val_datalist = [{\"image\": item[\"image\"]} for item in val_data.data if item[\"class_name\"] == \"validation\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKiaDOf35x1t",
        "outputId": "c7e4d5b1-8111-493b-a076-ec8aa239281c"
      },
      "outputs": [],
      "source": [
        "len(val_datalist)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zWdiovrjKes1"
      },
      "outputs": [],
      "source": [
        "val_transforms = transforms.Compose(\n",
        "    [\n",
        "        transforms.LoadImageD(keys=[\"image\"]),\n",
        "        transforms.EnsureChannelFirstd(keys=[\"image\"]),\n",
        "        transforms.SqueezeDimd(keys=[\"image\"], dim=-1),\n",
        "        transforms.Resized(keys=[\"image\"],spatial_size=(max_res, max_res)),\n",
        "        transforms.NormalizeIntensityd(keys=[\"image\"]),\n",
        "        transforms.CopyItemsd(keys=[\"image\"], times=1, names=[\"low_res_image\"]),\n",
        "        transforms.Resized(keys=[\"low_res_image\"], spatial_size=(max_res/4,max_res/4)),\n",
        "    ]\n",
        ")\n",
        "val_ds = Dataset(data=val_datalist, transform=val_transforms)\n",
        "val_loader = DataLoader(val_ds, batch_size=5, shuffle=True, num_workers=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample = val_ds[15]\n",
        "original_image = sample[\"image\"]\n",
        "low_res_image = sample[\"low_res_image\"]\n",
        "\n",
        "plt.imshow(original_image[0, :, :], cmap=\"gray\")\n",
        "plt.title(\"Original Image\")\n",
        "plt.show()\n",
        "\n",
        "plt.imshow(low_res_image[0,:,:], cmap=\"gray\")\n",
        "plt.title(\"Low resolution Image\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6KvUQUyKnee",
        "outputId": "d3d23e92-9c92-472b-e6be-f5a5d53dc061"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bVYA6sP3KsTw"
      },
      "outputs": [],
      "source": [
        "# Define autoencoder\n",
        "autoencoderkl = AutoencoderKL(\n",
        "    spatial_dims=2,\n",
        "    in_channels=1,\n",
        "    out_channels=1,\n",
        "    num_channels=(128, 256, 256),\n",
        "    latent_channels=3,\n",
        "    num_res_blocks=1,\n",
        "    norm_num_groups=16,\n",
        "    attention_levels=(False, False, True),\n",
        ")\n",
        "autoencoderkl = autoencoderkl.to(device)\n",
        "\n",
        "discriminator = PatchDiscriminator(spatial_dims=2, in_channels=1, num_layers_d=3, num_channels=64)\n",
        "discriminator = discriminator.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TkDsfaG4K61z",
        "outputId": "92963b73-6a42-4399-8843-ba7e4552f014"
      },
      "outputs": [],
      "source": [
        "perceptual_loss = PerceptualLoss(spatial_dims=2, network_type=\"alex\")\n",
        "perceptual_loss.to(device)\n",
        "perceptual_weight = 0.002\n",
        "\n",
        "adv_loss = PatchAdversarialLoss(criterion=\"least_squares\")\n",
        "adv_weight = 0.005\n",
        "\n",
        "optimizer_g = torch.optim.Adam(autoencoderkl.parameters(), lr=5e-5)\n",
        "optimizer_d = torch.optim.Adam(discriminator.parameters(), lr=1e-4)\n",
        "\n",
        "# %%\n",
        "scaler_g = GradScaler()\n",
        "scaler_d = GradScaler()\n",
        "accumulation_steps = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "unEEPxz79Qs6"
      },
      "outputs": [],
      "source": [
        "import gc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 667
        },
        "id": "E2hVs0jJLDXl",
        "outputId": "23e6f08d-5868-4c1c-a6f6-60cd37fa4385"
      },
      "outputs": [],
      "source": [
        " # ## Train Autoencoder\n",
        "\n",
        "# %%\n",
        "kl_weight = 1e-6\n",
        "n_epochs = 75\n",
        "val_interval = 10\n",
        "autoencoder_warm_up_n_epochs = 10\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    autoencoderkl.train()\n",
        "    discriminator.train()\n",
        "    epoch_loss = 0\n",
        "    gen_epoch_loss = 0\n",
        "    disc_epoch_loss = 0\n",
        "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), ncols=110)\n",
        "    progress_bar.set_description(f\"Epoch {epoch}\")\n",
        "    for step, batch in progress_bar:\n",
        "        images = batch[\"image\"].to(device)\n",
        "        optimizer_g.zero_grad(set_to_none=True)\n",
        "\n",
        "        with autocast(enabled=True):\n",
        "            reconstruction, z_mu, z_sigma = autoencoderkl(images)\n",
        "\n",
        "            recons_loss = F.l1_loss(reconstruction.float(), images.float())\n",
        "            p_loss = perceptual_loss(reconstruction.float(), images.float())\n",
        "            kl_loss = 0.5 * torch.sum(z_mu.pow(2) + z_sigma.pow(2) - torch.log(z_sigma.pow(2)) - 1, dim=[1, 2, 3])\n",
        "            kl_loss = torch.sum(kl_loss) / kl_loss.shape[0]\n",
        "            loss_g = recons_loss + (kl_weight * kl_loss) + (perceptual_weight * p_loss)\n",
        "\n",
        "            if epoch > autoencoder_warm_up_n_epochs:\n",
        "                logits_fake = discriminator(reconstruction.contiguous().float())[-1]\n",
        "                generator_loss = adv_loss(logits_fake, target_is_real=True, for_discriminator=False)\n",
        "                loss_g += adv_weight * generator_loss\n",
        "\n",
        "        scaler_g.scale(loss_g).backward()\n",
        "        scaler_g.step(optimizer_g)\n",
        "        scaler_g.update()\n",
        "\n",
        "        if epoch > autoencoder_warm_up_n_epochs:\n",
        "            optimizer_d.zero_grad(set_to_none=True)\n",
        "\n",
        "            with autocast(enabled=True):\n",
        "                logits_fake = discriminator(reconstruction.contiguous().detach())[-1]\n",
        "                loss_d_fake = adv_loss(logits_fake, target_is_real=False, for_discriminator=True)\n",
        "                logits_real = discriminator(images.contiguous().detach())[-1]\n",
        "                loss_d_real = adv_loss(logits_real, target_is_real=True, for_discriminator=True)\n",
        "                discriminator_loss = (loss_d_fake + loss_d_real) * 0.5\n",
        "\n",
        "                loss_d = adv_weight * discriminator_loss\n",
        "\n",
        "            scaler_d.scale(loss_d).backward()\n",
        "            scaler_d.step(optimizer_d)\n",
        "            scaler_d.update()\n",
        "\n",
        "        epoch_loss += recons_loss.item()\n",
        "        if epoch > autoencoder_warm_up_n_epochs:\n",
        "            gen_epoch_loss += generator_loss.item()\n",
        "            disc_epoch_loss += discriminator_loss.item()\n",
        "\n",
        "        progress_bar.set_postfix(\n",
        "            {\n",
        "                \"recons_loss\": epoch_loss / (step + 1),\n",
        "                \"gen_loss\": gen_epoch_loss / (step + 1),\n",
        "                \"disc_loss\": disc_epoch_loss / (step + 1),\n",
        "            }\n",
        "        )\n",
        "\n",
        "    if (epoch + 1) % val_interval == 0:\n",
        "        autoencoderkl.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for val_step, batch in enumerate(val_loader, start=1):\n",
        "                images = batch[\"image\"].to(device)\n",
        "                reconstruction, z_mu, z_sigma = autoencoderkl(images)\n",
        "                recons_loss = F.l1_loss(images.float(), reconstruction.float())\n",
        "                val_loss += recons_loss.item()\n",
        "\n",
        "        val_loss /= val_step\n",
        "        print(f\"epoch {epoch + 1} val loss: {val_loss:.4f}\")\n",
        "\n",
        "        # ploting reconstruction\n",
        "        plt.figure(figsize=(2, 2))\n",
        "        plt.imshow(torch.cat([images[0, 0].cpu(), reconstruction[0, 0].cpu()], dim=1), vmin=0, vmax=1, cmap=\"gray\")\n",
        "        plt.tight_layout()\n",
        "        plt.axis(\"off\")\n",
        "        plt.show()\n",
        "\n",
        "progress_bar.close()\n",
        "\n",
        "del discriminator\n",
        "del perceptual_loss\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ilun-3fXOKJn",
        "outputId": "d9144077-40a6-4c7e-d316-e81ac8e082b5"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    with autocast(enabled=True):\n",
        "        z = autoencoderkl.encode_stage_2_inputs(check_data[\"image\"].to(device))\n",
        "\n",
        "print(f\"Scaling factor set to {1/torch.std(z)}\")\n",
        "scale_factor = 1 / torch.std(z)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zc3TpWDdOK3e"
      },
      "outputs": [],
      "source": [
        "# ## Train Diffusion Model\n",
        "#\n",
        "# In order to train the diffusion model to perform super-resolution, we will need to concatenate the latent representation of the high-resolution with the low-resolution image. For this, we create a Diffusion model with `in_channels=4`. Since only the outputted latent representation is interesting, we set `out_channels=3`.\n",
        "\n",
        "# %%\n",
        "unet = DiffusionModelUNet(\n",
        "    spatial_dims=2,\n",
        "    in_channels=4,\n",
        "    out_channels=3,\n",
        "    num_res_blocks=1,\n",
        "    num_channels=(128, 128, 256, 512),\n",
        "    attention_levels=(False, False, True, True),\n",
        "    num_head_channels=(0, 0, 64, 64),\n",
        ")\n",
        "unet = unet.to(device)\n",
        "\n",
        "scheduler = DDPMScheduler(num_train_timesteps=1000, schedule=\"linear_beta\", beta_start=0.0015, beta_end=0.0195)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6n1poJZ3OOBI"
      },
      "outputs": [],
      "source": [
        "# %%\n",
        "low_res_scheduler = DDPMScheduler(num_train_timesteps=1000, schedule=\"linear_beta\", beta_start=0.0015, beta_end=0.0195)\n",
        "\n",
        "max_noise_level = 350"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 809
        },
        "id": "QS-A8fjxORvP",
        "outputId": "b96a4068-cb7e-402a-8d9d-ff766fb8a678"
      },
      "outputs": [],
      "source": [
        "# %%\n",
        "optimizer = torch.optim.Adam(unet.parameters(), lr=5e-5)\n",
        "\n",
        "scaler_diffusion = GradScaler()\n",
        "\n",
        "n_epochs = 200\n",
        "val_interval = 20\n",
        "epoch_loss_list = []\n",
        "val_epoch_loss_list = []\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    unet.train()\n",
        "    autoencoderkl.eval()\n",
        "    epoch_loss = 0\n",
        "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), ncols=110)\n",
        "    progress_bar.set_description(f\"Epoch {epoch}\")\n",
        "    for step, batch in progress_bar:\n",
        "        images = batch[\"image\"].to(device)\n",
        "        low_res_image = batch[\"low_res_image\"].to(device)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        with autocast(enabled=True):\n",
        "            with torch.no_grad():\n",
        "                latent = autoencoderkl.encode_stage_2_inputs(images) * scale_factor\n",
        "\n",
        "            # Noise augmentation\n",
        "            noise = torch.randn_like(latent).to(device)\n",
        "            low_res_noise = torch.randn_like(low_res_image).to(device)\n",
        "            timesteps = torch.randint(0, scheduler.num_train_timesteps, (latent.shape[0],), device=latent.device).long()\n",
        "            low_res_timesteps = torch.randint(\n",
        "                0, max_noise_level, (low_res_image.shape[0],), device=low_res_image.device\n",
        "            ).long()\n",
        "\n",
        "            noisy_latent = scheduler.add_noise(original_samples=latent, noise=noise, timesteps=timesteps)\n",
        "            noisy_low_res_image = scheduler.add_noise(\n",
        "                original_samples=low_res_image, noise=low_res_noise, timesteps=low_res_timesteps\n",
        "            )\n",
        "\n",
        "            latent_model_input = torch.cat([noisy_latent, noisy_low_res_image], dim=1)\n",
        "\n",
        "            noise_pred = unet(x=latent_model_input, timesteps=timesteps, class_labels=low_res_timesteps)\n",
        "            loss = F.mse_loss(noise_pred.float(), noise.float())\n",
        "\n",
        "        scaler_diffusion.scale(loss).backward()\n",
        "        scaler_diffusion.step(optimizer)\n",
        "        scaler_diffusion.update()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        progress_bar.set_postfix({\"loss\": epoch_loss / (step + 1)})\n",
        "    epoch_loss_list.append(epoch_loss / (step + 1))\n",
        "\n",
        "    if (epoch + 1) % val_interval == 0:\n",
        "        unet.eval()\n",
        "        val_loss = 0\n",
        "        for val_step, batch in enumerate(val_loader, start=1):\n",
        "            images = batch[\"image\"].to(device)\n",
        "            low_res_image = batch[\"low_res_image\"].to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                with autocast(enabled=True):\n",
        "                    latent = autoencoderkl.encode_stage_2_inputs(images) * scale_factor\n",
        "                    # Noise augmentation\n",
        "                    noise = torch.randn_like(latent).to(device)\n",
        "                    low_res_noise = torch.randn_like(low_res_image).to(device)\n",
        "                    timesteps = torch.randint(\n",
        "                        0, scheduler.num_train_timesteps, (latent.shape[0],), device=latent.device\n",
        "                    ).long()\n",
        "                    low_res_timesteps = torch.randint(\n",
        "                        0, max_noise_level, (low_res_image.shape[0],), device=low_res_image.device\n",
        "                    ).long()\n",
        "\n",
        "                    noisy_latent = scheduler.add_noise(original_samples=latent, noise=noise, timesteps=timesteps)\n",
        "                    noisy_low_res_image = scheduler.add_noise(\n",
        "                        original_samples=low_res_image, noise=low_res_noise, timesteps=low_res_timesteps\n",
        "                    )\n",
        "\n",
        "                    latent_model_input = torch.cat([noisy_latent, noisy_low_res_image], dim=1)\n",
        "                    noise_pred = unet(x=latent_model_input, timesteps=timesteps, class_labels=low_res_timesteps)\n",
        "                    loss = F.mse_loss(noise_pred.float(), noise.float())\n",
        "\n",
        "            val_loss += loss.item()\n",
        "        val_loss /= val_step\n",
        "        val_epoch_loss_list.append(val_loss)\n",
        "        print(f\"Epoch {epoch} val loss: {val_loss:.4f}\")\n",
        "\n",
        "        # Sampling image during training\n",
        "        sampling_image = low_res_image[0].unsqueeze(0)\n",
        "        latents = torch.randn((1, 3, max_res//4, max_res//4)).to(device)\n",
        "        low_res_noise = torch.randn((1, 1, max_res//4, max_res//4)).to(device)\n",
        "        noise_level = 20\n",
        "        noise_level = torch.Tensor((noise_level,)).long().to(device)\n",
        "        noisy_low_res_image = scheduler.add_noise(\n",
        "            original_samples=sampling_image,\n",
        "            noise=low_res_noise,\n",
        "            timesteps=torch.Tensor((noise_level,)).long().to(device),\n",
        "        )\n",
        "\n",
        "        scheduler.set_timesteps(num_inference_steps=1000)\n",
        "        for t in tqdm(scheduler.timesteps, ncols=110):\n",
        "            with torch.no_grad():\n",
        "                with autocast(enabled=True):\n",
        "                    latent_model_input = torch.cat([latents, noisy_low_res_image], dim=1)\n",
        "                    noise_pred = unet(\n",
        "                        x=latent_model_input, timesteps=torch.Tensor((t,)).to(device), class_labels=noise_level\n",
        "                    )\n",
        "                latents, _ = scheduler.step(noise_pred, t, latents)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            decoded = autoencoderkl.decode_stage_2_outputs(latents / scale_factor)\n",
        "\n",
        "        low_res_bicubic = nn.functional.interpolate(sampling_image, (max_res, max_res), mode=\"bicubic\")\n",
        "        plt.figure(figsize=(2, 2))\n",
        "        plt.style.use(\"default\")\n",
        "        plt.imshow(\n",
        "            torch.cat([images[0, 0].cpu(), low_res_bicubic[0, 0].cpu(), decoded[0, 0].cpu()], dim=1),\n",
        "            vmin=0,\n",
        "            vmax=1,\n",
        "            cmap=\"gray\",\n",
        "        )\n",
        "        plt.tight_layout()\n",
        "        plt.axis(\"off\")\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hmiyqQkPOVXg"
      },
      "outputs": [],
      "source": [
        "# ### Plotting sampling example\n",
        "\n",
        "# %%\n",
        "# Sampling image during training\n",
        "unet.eval()\n",
        "num_samples = 5\n",
        "validation_batch = first(val_loader)\n",
        "\n",
        "images = validation_batch[\"image\"].to(device)\n",
        "sampling_image = validation_batch[\"low_res_image\"].to(device)[:num_samples]\n",
        "\n",
        "# %%\n",
        "latents = torch.randn((num_samples, 3, 16, 16)).to(device)\n",
        "low_res_noise = torch.randn((num_samples, 1, 16, 16)).to(device)\n",
        "noise_level = 10\n",
        "noise_level = torch.Tensor((noise_level,)).long().to(device)\n",
        "noisy_low_res_image = scheduler.add_noise(\n",
        "    original_samples=sampling_image, noise=low_res_noise, timesteps=torch.Tensor((noise_level,)).long().to(device)\n",
        ")\n",
        "scheduler.set_timesteps(num_inference_steps=1000)\n",
        "for t in tqdm(scheduler.timesteps, ncols=110):\n",
        "    with torch.no_grad():\n",
        "        with autocast(enabled=True):\n",
        "            latent_model_input = torch.cat([latents, noisy_low_res_image], dim=1)\n",
        "            noise_pred = unet(x=latent_model_input, timesteps=torch.Tensor((t,)).to(device), class_labels=noise_level)\n",
        "\n",
        "        # 2. compute previous image: x_t -> x_t-1\n",
        "        latents, _ = scheduler.step(noise_pred, t, latents)\n",
        "\n",
        "with torch.no_grad():\n",
        "    decoded = autoencoderkl.decode_stage_2_outputs(latents / scale_factor)\n",
        "\n",
        "# %%\n",
        "low_res_bicubic = nn.functional.interpolate(sampling_image, (64, 64), mode=\"bicubic\")\n",
        "fig, axs = plt.subplots(num_samples, 3, figsize=(8, 8))\n",
        "axs[0, 0].set_title(\"Original image\")\n",
        "axs[0, 1].set_title(\"Low-resolution Image\")\n",
        "axs[0, 2].set_title(\"Outputted image\")\n",
        "for i in range(0, num_samples):\n",
        "    axs[i, 0].imshow(images[i, 0].cpu(), vmin=0, vmax=1, cmap=\"gray\")\n",
        "    axs[i, 0].axis(\"off\")\n",
        "    axs[i, 1].imshow(low_res_bicubic[i, 0].cpu(), vmin=0, vmax=1, cmap=\"gray\")\n",
        "    axs[i, 1].axis(\"off\")\n",
        "    axs[i, 2].imshow(decoded[i, 0].cpu(), vmin=0, vmax=1, cmap=\"gray\")\n",
        "    axs[i, 2].axis(\"off\")\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CmwmIDwfMl4m"
      },
      "outputs": [],
      "source": [
        "if directory is None:\n",
        "  shutil.rmtree(root_dir)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
